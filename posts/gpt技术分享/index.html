<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="GPT技术分享 # 先介绍几个名词 # GPT 和 ChatGPT # Generative Pre-Training Transformer 是一种基于Transformer架构的预训练语言模型.
我们所接触到的chatgpt是针对gpt3.5模型的一种应用实例, 如今chatgpt已经为会员用户切换为gpt4.0模型. 未来短期时间内应该不会开放给免费用户使用.
GPT模型本质是续写模型, 它根据输入的文本生成相应的续写文本, 从而实现对话、文章等自然语言文本的生成. 所以其并不具备实际的创新能力, 只能对以往的“经验”做出梳理.
LLMs # Large language models (LLMs)大语言模型, 包括GPT和GLM等, 是一个统称. 以下是这个统称下的不同模型架构：
主流预训练模型架构 # autoregressive自回归模型（AR模型）：代表作GPT. 本质上是一个left-to-right的语言模型. 通常用于生成式任务, 在长文本生成方面取得了巨大的成功, 比如自然语言生成（NLG）领域的任务：摘要、翻译或抽象问答. 当扩展到十亿级别参数时, 表现出了少样本学习能力. 缺点是单向注意力机制, 在NLU任务中, 无法完全捕捉上下文的依赖关系.
autoencoding自编码模型（AE模型）：代表作BERT. 是通过某个降噪目标（比如MLM）训练的双向文本编码器. 编码器会产出适用于NLU任务的上下文表示, 擅长领域主要是自然语言处理领域, 例如文本分类、命名实体识别、情感分析、问答系统等. 但无法直接用于文本生成.
encoder-decoder（Seq2seq模型）：代表作T5. 采用双向注意力机制, 通常用于条件生成任务, 比如文本摘要、机器翻译等.
GLM模型基于autoregressive blank infilling方法, 结合了上述三种预训练模型的思想, 代表有清华大学的ChatGLM
LoRA和Prompt # Prompt 提示词, 是一种用于指导模型生成输出的文本片段, Prompt通常包含一些关键词或短语, 用于提示模型生成特定类型的文本. Prompt的使用可以提高模型的生成效果和准确性, 特别是在处理特定领域的文本时. 通过使用Prompt, 模型可以更好地理解输入文本的含义和上下文信息, 从而生成更加符合要求的输出文本. LoRA英文全称Low-Rank Adaptation of Large Language Models, 直译为大语言模型的低阶适应, 是一种PEFT（参数高效性微调方法）, 这是微软的研究人员为了解决大语言模型微调而开发的一项技术."><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:title" content="GPT技术分享"><meta property="og:description" content="GPT技术分享 # 先介绍几个名词 # GPT 和 ChatGPT # Generative Pre-Training Transformer 是一种基于Transformer架构的预训练语言模型.
我们所接触到的chatgpt是针对gpt3.5模型的一种应用实例, 如今chatgpt已经为会员用户切换为gpt4.0模型. 未来短期时间内应该不会开放给免费用户使用.
GPT模型本质是续写模型, 它根据输入的文本生成相应的续写文本, 从而实现对话、文章等自然语言文本的生成. 所以其并不具备实际的创新能力, 只能对以往的“经验”做出梳理.
LLMs # Large language models (LLMs)大语言模型, 包括GPT和GLM等, 是一个统称. 以下是这个统称下的不同模型架构：
主流预训练模型架构 # autoregressive自回归模型（AR模型）：代表作GPT. 本质上是一个left-to-right的语言模型. 通常用于生成式任务, 在长文本生成方面取得了巨大的成功, 比如自然语言生成（NLG）领域的任务：摘要、翻译或抽象问答. 当扩展到十亿级别参数时, 表现出了少样本学习能力. 缺点是单向注意力机制, 在NLU任务中, 无法完全捕捉上下文的依赖关系.
autoencoding自编码模型（AE模型）：代表作BERT. 是通过某个降噪目标（比如MLM）训练的双向文本编码器. 编码器会产出适用于NLU任务的上下文表示, 擅长领域主要是自然语言处理领域, 例如文本分类、命名实体识别、情感分析、问答系统等. 但无法直接用于文本生成.
encoder-decoder（Seq2seq模型）：代表作T5. 采用双向注意力机制, 通常用于条件生成任务, 比如文本摘要、机器翻译等.
GLM模型基于autoregressive blank infilling方法, 结合了上述三种预训练模型的思想, 代表有清华大学的ChatGLM
LoRA和Prompt # Prompt 提示词, 是一种用于指导模型生成输出的文本片段, Prompt通常包含一些关键词或短语, 用于提示模型生成特定类型的文本. Prompt的使用可以提高模型的生成效果和准确性, 特别是在处理特定领域的文本时. 通过使用Prompt, 模型可以更好地理解输入文本的含义和上下文信息, 从而生成更加符合要求的输出文本. LoRA英文全称Low-Rank Adaptation of Large Language Models, 直译为大语言模型的低阶适应, 是一种PEFT（参数高效性微调方法）, 这是微软的研究人员为了解决大语言模型微调而开发的一项技术."><meta property="og:type" content="article"><meta property="og:url" content="http://example.org/posts/gpt%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-06-12T00:00:00+00:00"><meta property="article:modified_time" content="2022-06-12T00:00:00+00:00"><title>GPT技术分享 | Ian's Blog</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.c58292d36b18b675680ab9baea2029204537b839ea72f258746ec0f32ce8d6c8.css integrity="sha256-xYKS02sYtnVoCrm66iApIEU3uDnqcvJYdG7A8yzo1sg=" crossorigin=anonymous><script defer src=/flexsearch.min.js></script><script defer src=/en.search.min.967a0b86a468867bef105ea781246b02d3ec23457360efc5c19bf3896a2eec17.js integrity="sha256-lnoLhqRohnvvEF6ngSRrAtPsI0VzYO/FwZvziWou7Bc=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>Ian's Blog</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li class=book-section-flat><a href=/docs/programmer/>程序员笔记</a><ul><li><input type=checkbox id=section-883e27361d38e16afb68faff3435ac0b class=toggle>
<label for=section-883e27361d38e16afb68faff3435ac0b class="flex justify-between"><a role=button>机器学习</a></label><ul><li><a href=/docs/programmer/ml/stable-diffusion/>AI画图</a></li><li><a href=/docs/programmer/ml/%E5%9B%BE%E7%89%87%E8%83%8C%E6%99%AF%E6%B6%88%E9%99%A4/>图片背景消除</a></li><li><a href=/docs/programmer/ml/%E6%9C%BA%E5%99%A8%E4%BA%BA%E8%A1%8C%E4%B8%BA%E8%87%AA%E5%8A%A8%E7%BC%96%E6%8E%92%E5%92%8C%E4%BB%BB%E5%8A%A1/>机器人行为自动编排和任务</a></li><li><a href=/docs/programmer/ml/%E7%88%AC%E8%99%AB/>爬虫</a></li><li><a href=/docs/programmer/ml/paddle/>Paddle</a></li><li><a href=/docs/programmer/ml/tensorflow/>Tensorflow</a></li><li><a href=/docs/programmer/ml/opencv/>OpenCV</a></li><li><a href=/docs/programmer/ml/yolo/>Demo Test项目中的一些东西</a></li><li><a href=/docs/programmer/ml/python-%E5%9B%BE%E8%A1%A8/>Python 图表</a></li><li><a href=/docs/programmer/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E5%BA%93%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95/>机器学习库</a></li></ul></li><li><input type=checkbox id=section-8658298e10b544e890095f646916165a class=toggle>
<label for=section-8658298e10b544e890095f646916165a class="flex justify-between"><a href=/docs/programmer/cloudnative/>云原生</a></label><ul><li><input type=checkbox id=section-31d3b02fd4a132635e483e7e756058ea class=toggle>
<label for=section-31d3b02fd4a132635e483e7e756058ea class="flex justify-between"><a href=/docs/programmer/cloudnative/redis/>中间件</a></label><ul><li><a href=/docs/programmer/cloudnative/redis/python-redis-%E5%AE%A2%E6%88%B7%E7%AB%AF/>Python Redis 客户端</a></li><li><a href=/docs/programmer/cloudnative/redis/bigkey-and-hotkey/>大key、热key问题</a></li><li><a href=/docs/programmer/cloudnative/redis/the-basics-of-redis/>Redis基础</a></li><li><a href=/docs/programmer/cloudnative/redis/advanced-knowledge-of-redis/>Redis进阶</a></li><li><a href=/docs/programmer/cloudnative/redis/%E4%BD%BF%E7%94%A8%E5%91%BD%E4%BB%A4/>Redis常用命令</a></li></ul></li><li><input type=checkbox id=section-62d608ed890b3abc76dae78ccfcab912 class=toggle>
<label for=section-62d608ed890b3abc76dae78ccfcab912 class="flex justify-between"><a role=button>k8s</a></label><ul><li><a href=/docs/programmer/cloudnative/k8s/elk%E5%9C%A8k8s%E4%B8%8A%E7%9A%84%E9%83%A8%E7%BD%B2%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B/>elk在k8s上的部署使用示例</a></li><li><a href=/docs/programmer/cloudnative/k8s/k8s-%E9%85%8D%E5%A5%97%E8%AF%B4%E6%98%8E/>k8s 配套说明</a></li><li><a href=/docs/programmer/cloudnative/k8s/k8s-%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/>k8s技术分享</a></li><li><a href=/docs/programmer/cloudnative/k8s/k8s%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%92%8C%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%A7%A3%E6%9E%90/>k8s学习-常用命令和配置文件</a></li><li><a href=/docs/programmer/cloudnative/k8s/argo-workflow%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%92%8C%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%E5%88%86%E6%9E%90/>Argo Workflow性能测试和使用场景分析</a></li><li><a href=/docs/programmer/cloudnative/k8s/argo-%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/>Argo 使用记录</a></li></ul></li><li><input type=checkbox id=section-b828bf3d116bc282da9db25a06bf908e class=toggle>
<label for=section-b828bf3d116bc282da9db25a06bf908e class="flex justify-between"><a role=button>中间件</a></label><ul><li><a href=/docs/programmer/cloudnative/middleware/kafka-%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/>Kafka 安装和使用</a></li></ul></li><li><a href=/docs/programmer/cloudnative/elk/%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E7%B3%BB%E7%BB%9F/>日志收集系统: loki+promtail+Grafana</a></li><li><a href=/docs/programmer/cloudnative/%E5%AE%B9%E5%99%A8/note-of-podman/>Podman</a></li><li><a href=/docs/programmer/cloudnative/nginx%E5%AE%9E%E7%94%A8%E9%85%8D%E7%BD%AE/>Nginx实用配置</a></li><li><a href=/docs/programmer/cloudnative/uwsgi-%E5%A4%84%E7%90%86%E8%AE%B0%E5%BD%95/>uwsgi 处理记录</a></li><li><a href=/docs/programmer/cloudnative/note-of-docker/>Docker</a></li><li><a href=/docs/programmer/cloudnative/%E5%AE%B9%E5%99%A8/note-of-docker/>Docker</a></li><li><a href=/docs/programmer/cloudnative/fastapi/>Django的建站的(｡･･)ﾉﾞ</a></li><li><a href=/docs/programmer/cloudnative/nginx-%E9%AB%98%E5%8F%AF%E7%94%A8/>Nginx高可用</a></li><li><a href=/docs/programmer/cloudnative/notesdjango/>Django的建站的(｡･･)ﾉﾞ</a></li><li><a href=/docs/programmer/cloudnative/sonar-%E4%BB%A3%E7%A0%81%E9%9D%99%E6%80%81%E6%A3%80%E6%9F%A5/>Sonar 代码静态检查</a></li></ul></li><li><input type=checkbox id=section-bf4e0d6f0b81f7b3ec08ed1fc66b874d class=toggle>
<label for=section-bf4e0d6f0b81f7b3ec08ed1fc66b874d class="flex justify-between"><a href=/docs/programmer/langs/>编程语言</a></label><ul><li><input type=checkbox id=section-9f8ac8f06e138c7ac13ff61f23b4d497 class=toggle>
<label for=section-9f8ac8f06e138c7ac13ff61f23b4d497 class="flex justify-between"><a role=button>Golang</a></label><ul><li><a href=/docs/programmer/langs/golang/%E6%8A%8Agolang%E5%BD%93%E8%84%9A%E6%9C%AC%E8%AF%AD%E8%A8%80%E7%94%A8/>把Golang当脚本语言用</a></li><li><a href=/docs/programmer/langs/golang/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/>常用命令</a></li><li><a href=/docs/programmer/langs/golang/advanced-knowledge-of-golang/>Golang进阶笔记</a></li><li><a href=/docs/programmer/langs/golang/note-of-golang/>Golang笔记</a></li></ul></li><li><input type=checkbox id=section-771df6c720301e69f1715f7fc174ac3d class=toggle>
<label for=section-771df6c720301e69f1715f7fc174ac3d class="flex justify-between"><a role=button>Python</a></label><ul><li><a href=/docs/programmer/langs/python/%E8%A7%84%E8%8C%83/>Python编码规范</a></li><li><a href=/docs/programmer/langs/python/sqlalchemy/>SqlAlchemy - 数据库Orm</a></li><li><a href=/docs/programmer/langs/python/pypi/>PyPi使用说明</a></li><li><a href=/docs/programmer/langs/python/pytest/>PyTest</a></li><li><a href=/docs/programmer/langs/python/paramiko-%E4%BD%BF%E7%94%A8-sshsftp/>Paramiko 使用 Ssh&amp;sftp</a></li><li><a href=/docs/programmer/langs/python/fastapi/>FastAPI</a></li><li><a href=/docs/programmer/langs/python/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%A4%9A%E8%BF%9B%E7%A8%8B/>Python 多线程多进程</a></li><li><a href=/docs/programmer/langs/python/notespython/>Python 常用库</a></li><li><a href=/docs/programmer/langs/python/notespython/>Python 笔记</a></li><li><a href=/docs/programmer/langs/python/py%E5%B0%8F%E5%B7%A5%E5%85%B7%E5%92%8C%E5%8A%9F%E8%83%BD%E6%80%A7%E6%96%B9%E6%B3%95/>Py小工具和功能性方法</a></li><li><a href=/docs/programmer/langs/python/notespython/>解决问题</a></li></ul></li><li><a href=/docs/programmer/langs/cmake/>CMake 使用Tips</a></li><li><a href=/docs/programmer/langs/tips-of-debugers/>Tips of debuggers</a></li><li><a href=/docs/programmer/langs/tips-of-markdown/>Tips of MarkDown</a></li><li><a href=/docs/programmer/langs/java/notesjava/>愉快的Java(happy to learn the fuck java)</a></li><li><a href=/docs/programmer/langs/note-for-algo/>算法</a></li></ul></li><li><input type=checkbox id=section-d5f99046a51e5e750b61f2e037945fcc class=toggle>
<label for=section-d5f99046a51e5e750b61f2e037945fcc class="flex justify-between"><a role=button>基础工具和配置</a></label><ul><li><a href=/docs/programmer/basetc/%E7%BB%88%E7%AB%AF%E6%B5%8F%E8%A7%88%E5%99%A8/>终端浏览器</a></li><li><a href=/docs/programmer/basetc/curl/>curl</a></li><li><a href=/docs/programmer/basetc/%E6%96%87%E6%9C%AC%E4%B8%89%E5%89%91%E5%AE%A2/>文本三剑客</a></li><li><a href=/docs/programmer/basetc/tmux/>Tmux使用笔记</a></li><li><a href=/docs/programmer/basetc/obsidian%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE%E6%8F%92%E4%BB%B6/>Obsidian思维导图插件</a></li><li><a href=/docs/programmer/basetc/for_china/>各个软件换源</a></li><li><a href=/docs/programmer/basetc/tipsofvim/>tip Of vim</a></li><li><a href=/docs/programmer/basetc/editer/>编辑器使用</a></li><li><a href=/docs/programmer/basetc/bash/>Bash</a></li><li><a href=/docs/programmer/basetc/gitbook/>Gitbook</a></li><li><a href=/docs/programmer/basetc/vim/>Vim</a></li><li><a href=/docs/programmer/basetc/%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B/>supervisor</a></li></ul></li><li><input type=checkbox id=section-7e5360c5e7954906b897ed79085884b6 class=toggle>
<label for=section-7e5360c5e7954906b897ed79085884b6 class="flex justify-between"><a href=/docs/programmer/gui/>图形用户界面-GUI</a></label><ul><li><input type=checkbox id=section-d68552b374b14edaa2443cb0f45f94f7 class=toggle>
<label for=section-d68552b374b14edaa2443cb0f45f94f7 class="flex justify-between"><a href=/docs/programmer/gui/example/>Blog 构建</a></label><ul><li><input type=checkbox id=section-9610a10c098f6edce85f18e8d27ddc6d class=toggle>
<label for=section-9610a10c098f6edce85f18e8d27ddc6d class="flex justify-between"><a href=/docs/programmer/gui/example/table-of-contents/>Table of Contents</a></label><ul><li><a href=/docs/programmer/gui/example/table-of-contents/with-toc/>With ToC</a></li><li><a href=/docs/programmer/gui/example/table-of-contents/without-toc/>Without ToC</a></li></ul></li><li><input type=checkbox id=section-9ec2030fb4762791e574be1af10795d3 class=toggle>
<label for=section-9ec2030fb4762791e574be1af10795d3 class="flex justify-between"><a href=/docs/programmer/gui/example/shortcodes/>Shortcodes</a></label><ul><li><a href=/docs/programmer/gui/example/shortcodes/buttons/>Buttons</a></li><li><a href=/docs/programmer/gui/example/shortcodes/columns/>Columns</a></li><li><a href=/docs/programmer/gui/example/shortcodes/details/>Details</a></li><li><a href=/docs/programmer/gui/example/shortcodes/expand/>Expand</a></li><li><a href=/docs/programmer/gui/example/shortcodes/hints/>Hints</a></li><li><input type=checkbox id=section-0a13810fcdbb2cfbfd8459e55601b351 class=toggle>
<label for=section-0a13810fcdbb2cfbfd8459e55601b351 class="flex justify-between"><a href=/docs/programmer/gui/example/shortcodes/section/>Section</a></label><ul><li><a href=/docs/programmer/gui/example/shortcodes/section/first-page/>First Page</a></li><li><a href=/docs/programmer/gui/example/shortcodes/section/second-page/>Second Page</a></li></ul></li><li><a href=/docs/programmer/gui/example/shortcodes/tabs/>Tabs</a></li></ul></li></ul></li><li><a href=/docs/programmer/gui/ffmpeg/>ffmpeg 使用</a></li><li><a href=/docs/programmer/gui/pyinstaller-python%E6%89%93%E5%8C%85/>python打包</a></li><li><a href=/docs/programmer/gui/qt/>Qt/PySide</a></li><li><a href=/docs/programmer/gui/vn.py%E7%AC%94%E8%AE%B0-%E4%BA%A4%E6%98%93%E5%B9%B3%E5%8F%B0%E5%AE%A2%E6%88%B7%E7%AB%AF/>Vn.Py学习笔记（Python交易平台框架）</a></li><li><a href=/docs/programmer/gui/notespython/>图形化界面 （Python Gui）</a></li></ul></li><li><input type=checkbox id=section-0bf4f4329214e20fa67ca3e12c6aad0c class=toggle>
<label for=section-0bf4f4329214e20fa67ca3e12c6aad0c class="flex justify-between"><a role=button>平台</a></label><ul><li><a href=/docs/programmer/platforms/%E9%80%86%E5%90%91/>逆向三板斧</a></li><li><a href=/docs/programmer/platforms/os/steam/>Steam</a></li><li><a href=/docs/programmer/platforms/android/waydroid/>Linux 跑安卓</a></li><li><a href=/docs/programmer/platforms/os/ubuntu/>Ubuntu</a></li><li><a href=/docs/programmer/platforms/%E9%98%BF%E9%87%8C%E4%BA%91%E4%BD%BF%E7%94%A8/>阿里云使用</a></li><li><a href=/docs/programmer/platforms/wps-for-linux/>WPS for Linux</a></li><li><a href=/docs/programmer/platforms/appsflyer/>AppsFlyer-外网移动归因营销分析平台</a></li><li><a href=/docs/programmer/platforms/install_some/>安装问题</a></li><li><a href=/docs/programmer/platforms/android/android/>安卓</a></li><li><a href=/docs/programmer/platforms/os/freebsd/>FreeBSD</a></li><li><a href=/docs/programmer/platforms/os/note-of-linux/>Linux 笔记</a></li><li><a href=/docs/programmer/platforms/android/adb/>adb</a></li><li><a href=/docs/programmer/platforms/git/>Git</a></li><li><a href=/docs/programmer/platforms/os/ros/>ROS机器人操作系统</a></li><li><a href=/docs/programmer/platforms/os/problem-of-windows/>Windows 爬坑记</a></li><li><a href=/docs/programmer/platforms/tips-of-problems/>解决问题记录笔记</a></li></ul></li><li><input type=checkbox id=section-4446dd07527142b855f26d7cc8f0e617 class=toggle>
<label for=section-4446dd07527142b855f26d7cc8f0e617 class="flex justify-between"><a href=/docs/programmer/database/>Database</a></label><ul><li><a href=/docs/programmer/database/note-of-db-data-mariadb/>数据库-MariaDB篇</a></li><li><a href=/docs/programmer/database/dgraph-graph-db/>Dgraph使用小记</a></li><li><a href=/docs/programmer/database/note-of-db-data-mongodb/>数据库-MongoDB篇</a></li><li><a href=/docs/programmer/database/note-of-db-data-mysql/>数据库-MySQL篇</a></li></ul></li><li><input type=checkbox id=section-de7bfad1d124522974cdf8addfbb23f2 class=toggle>
<label for=section-de7bfad1d124522974cdf8addfbb23f2 class="flex justify-between"><a role=button>Net</a></label><ul><li><a href=/docs/programmer/net/tools/ngrok/>ngrok - 一个免费内网穿透api工具</a></li><li><a href=/docs/programmer/net/crawler/mitmproxy/>mitmproxy</a></li><li><a href=/docs/programmer/net/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/>网络编程</a></li><li><a href=/docs/programmer/net/nginx%E5%AE%9E%E7%94%A8%E9%85%8D%E7%BD%AE/>Nginx实用配置</a></li><li><a href=/docs/programmer/net/tips-of-grpc/>gRpc使用小记</a></li><li><a href=/docs/programmer/net/epoll%E5%AE%9E%E7%8E%B0/>Epoll实现</a></li></ul></li><li><input type=checkbox id=section-d325c59fc6513e1b1e05a60b192d4973 class=toggle>
<label for=section-d325c59fc6513e1b1e05a60b192d4973 class="flex justify-between"><a href=/docs/programmer/hardware/>硬件</a></label><ul><li><a href=/docs/programmer/hardware/raspberrypi/>Raspberry Pi</a></li><li><a href=/docs/programmer/hardware/screen/>Screen</a></li></ul></li></ul></li><li class=book-section-flat><span>建模和游戏</span><ul><li><a href=/docs/3dgame/blender/>Blender</a></li><li><a href=/docs/3dgame/noteofue4/>UE4 笔记</a></li></ul></li></ul><ul><li><a href=/posts/>Blog</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>GPT技术分享</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#先介绍几个名词>先介绍几个名词</a><ul><li><a href=#gpt-和-chatgpt>GPT 和 ChatGPT</a></li><li><a href=#llms>LLMs</a></li><li><a href=#lora和prompt>LoRA和Prompt</a></li></ul></li><li><a href=#llms如何产生思维>LLMs如何产生“思维”</a><ul><li><a href=#我们如何使用>我们如何使用</a></li><li><a href=#目前一些有有影响力的开源模型>目前一些有有影响力的开源模型</a></li></ul></li><li><a href=#privategpt-和-localgpt>privateGPT 和 localGPT</a></li><li><a href=#结合公司使用场景分析>结合公司使用场景分析</a></li></ul></nav></aside></header><article class=markdown><h1><a href=/posts/gpt%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/>GPT技术分享</a></h1><h5>June 12, 2022</h5><h1 id=gpt技术分享>GPT技术分享
<a class=anchor href=#gpt%e6%8a%80%e6%9c%af%e5%88%86%e4%ba%ab>#</a></h1><h2 id=先介绍几个名词>先介绍几个名词
<a class=anchor href=#%e5%85%88%e4%bb%8b%e7%bb%8d%e5%87%a0%e4%b8%aa%e5%90%8d%e8%af%8d>#</a></h2><h3 id=gpt-和-chatgpt>GPT 和 ChatGPT
<a class=anchor href=#gpt-%e5%92%8c-chatgpt>#</a></h3><p><code>Generative Pre-Training Transformer</code> 是一种基于<code>Transformer</code>架构的预训练语言模型.</p><p>我们所接触到的<code>chatgpt</code>是针对<code>gpt3.5</code>模型的一种应用实例, 如今<code>chatgpt</code>已经为会员用户切换为<code>gpt4.0</code>模型. 未来短期时间内应该不会开放给免费用户使用.</p><p><code>GPT</code>模型本质是续写模型, 它根据输入的文本生成相应的续写文本, 从而实现对话、文章等自然语言文本的生成. 所以其并不具备实际的创新能力, 只能对以往的“经验”做出梳理.</p><h3 id=llms>LLMs
<a class=anchor href=#llms>#</a></h3><p><code>Large language models (LLMs)</code>大语言模型, 包括<code>GPT</code>和<code>GLM</code>等, 是一个统称. 以下是这个统称下的不同模型架构：</p><h4 id=主流预训练模型架构>主流预训练模型架构
<a class=anchor href=#%e4%b8%bb%e6%b5%81%e9%a2%84%e8%ae%ad%e7%bb%83%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84>#</a></h4><ol><li><p><code>autoregressive</code>自回归模型（AR模型）：代表作<code>GPT</code>. 本质上是一个<code>left-to-right</code>的语言模型. 通常用于生成式任务, 在长文本生成方面取得了巨大的成功, 比如自然语言生成（<code>NLG</code>）领域的任务：摘要、翻译或抽象问答. 当扩展到十亿级别参数时, 表现出了少样本学习能力. 缺点是<code>单向注意力机制</code>, 在NLU任务中, 无法完全捕捉上下文的依赖关系.</p></li><li><p><code>autoencoding</code>自编码模型（AE模型）：代表作BERT. 是通过某个降噪目标（比如MLM）训练的双向文本编码器. 编码器会产出适用于NLU任务的上下文表示, 擅长领域主要是自然语言处理领域, 例如文本分类、命名实体识别、情感分析、问答系统等. 但无法直接用于文本生成.</p></li><li><p><code>encoder-decoder</code>（Seq2seq模型）：代表作T5. 采用双向注意力机制, 通常用于条件生成任务, 比如文本摘要、机器翻译等.</p></li><li><p>GLM模型基于<code>autoregressive blank infilling</code>方法, 结合了上述三种预训练模型的思想, 代表有清华大学的<code>ChatGLM</code></p></li></ol><h3 id=lora和prompt>LoRA和Prompt
<a class=anchor href=#lora%e5%92%8cprompt>#</a></h3><ol><li><code>Prompt</code> 提示词, 是一种用于指导模型生成输出的文本片段, <code>Prompt</code>通常包含一些关键词或短语, 用于提示模型生成特定类型的文本. <code>Prompt</code>的使用可以提高模型的生成效果和准确性, 特别是在处理特定领域的文本时. <code>通过使用Prompt</code>, 模型可以更好地理解输入文本的含义和上下文信息, 从而生成更加符合要求的输出文本.</li><li><code>LoRA</code>英文全称<code>Low-Rank Adaptation of Large Language Models</code>, 直译为大语言模型的低阶适应, 是一种PEFT（参数高效性微调方法）, 这是微软的研究人员为了解决大语言模型微调而开发的一项技术. 当然参数高效性微调方法中实现最简单的方法还是<code>Prompt tuning</code>, 固定模型前馈层参数, 仅仅更新部分embedding参数即可实现低成本微调大模型</li></ol><h2 id=llms如何产生思维>LLMs如何产生“思维”
<a class=anchor href=#llms%e5%a6%82%e4%bd%95%e4%ba%a7%e7%94%9f%e6%80%9d%e7%bb%b4>#</a></h2><p>这是在训练数据达到一定规模后, LLMs模型开始表现出一种现象. 应该有论文讨论这个, 没有细看. 不过如今模型所表现出的效果都是佐以大量的指令微调, 才得以最终呈现出来.</p><h3 id=我们如何使用>我们如何使用
<a class=anchor href=#%e6%88%91%e4%bb%ac%e5%a6%82%e4%bd%95%e4%bd%bf%e7%94%a8>#</a></h3><p>因为并不能直接使用, 即使直接使用ChatGPT的API也需要对其进行一定的能力限制和格式化. 主要通过<code>Prompt(提示词)</code>来实现. 如果使用<code>GPT-4</code>或者<code>GPT3.5</code>接口的话, 可以仅通过调整和开发提示词来实现比较好的效果.</p><p>但如果想要保证数据安全, 那必须使用可本地化部署的大语言模型. 如下《目前一些有有影响力的开源模型》中都是可本地化部署的大模型. 但是就使用体验来说, 距离<code>ChatGPT</code>都有比较大的差距. 所以使用的话需要做垂类的微调, 即使用<code>LoRA</code>.</p><p><a href=https://huggingface.co/blog/lora>LoRA</a>, 是微软的研究人员为了解决大语言模型微调而开发的一项技术. 比如, GPT-3有1750亿参数, 为了让它能干特定领域的活儿, 需要做微调, 但是如果直接对GPT-3做微调, 成本太高太麻烦了. LoRA的做法是, 冻结预训练好的模型权重参数, 然后在每个Transformer（Transforme就是GPT的那个T）块里注入可训练的层, 由于不需要对模型的权重参数重新计算梯度, 所以, 大大减少了需要训练的计算量. 研究发现, LoRA的微调质量与全模型微调相当. 要做个比喻的话, 就好比是大模型的一个小模型, 或者说是一个插件.</p><h3 id=目前一些有有影响力的开源模型>目前一些有有影响力的开源模型
<a class=anchor href=#%e7%9b%ae%e5%89%8d%e4%b8%80%e4%ba%9b%e6%9c%89%e6%9c%89%e5%bd%b1%e5%93%8d%e5%8a%9b%e7%9a%84%e5%bc%80%e6%ba%90%e6%a8%a1%e5%9e%8b>#</a></h3><ul><li>Vicuna: a chat assistant fine-tuned from LLaMA on user-shared conversations by LMSYS</li><li>WizardLM: an instruction-following LLM using evol-instruct by Microsoft</li><li>GPT4All-Snoozy: A finetuned LLaMA model on assistant style data by Nomic AI</li><li>Guanaco: a model fine-tuned with QLoRA by UW</li><li>Koala: a dialogue model for academic research by BAIR</li><li>RWKV-4-Raven: an RNN with transformer-level LLM performance</li><li>Alpaca: a model fine-tuned from LLaMA on instruction-following demonstrations by Stanford</li><li>ChatGLM: an open bilingual dialogue language model by Tsinghua University</li><li>OpenAssistant (oasst): an Open Assistant for everyone by LAION</li><li>LLaMA: open and efficient foundation language models by Meta Dolly: an instruction-tuned open large language model by Databricks</li><li>StableLM: Stability AI language models</li><li>MPT-Chat: a chatbot fine-tuned from MPT-7B by MosaicML</li><li>FastChat-T5: a chat assistant fine-tuned from FLAN-T5 by LMSYS</li><li>ChatYuan: 是由元语智能开发团队开发和发布的, 自称第一个国内最早的一个功能型对话大模型. 从披露的技术细节看, 底层采用7亿参数规模的T5模型, 并基于PromptClue进行了监督微调形成了ChatYuan</li></ul><h2 id=privategpt-和-localgpt>privateGPT 和 localGPT
<a class=anchor href=#privategpt-%e5%92%8c-localgpt>#</a></h2><p>如果在我们的使用场景中, 需要进一步规范化和对输出结果有更可控的预期. 可以参考<code>privateGPT</code>和<code>localGPT</code>的方案. 这两者都不是模型, 而是一种大模型的使用方式.</p><p>其中<code>privateGPT</code>使用<code>GPT4All</code>大模型, <code>localGPT</code>使用
<code>Vicuna</code>大模型, 当然, 这<code>localGPT</code>本真就是基于<code>privateGPT</code>的架构和设计思路派生的, 我们完全可以使用<code>ChatGLM</code>模型用来增强中文能力——这种方式比使用<code>GPT4ALL</code>大模型借用<code>Embeddings Model</code>实现的中文支持效果要好.</p><p><img src=localgpt.png alt=localgpt架构></p><p>如上架构图所示, 这个架构分为两步式, 首先将我们提供的文档存储为知识库. 之后收到问题, 再将问题转换, 随后对存储文档进行搜索, 并对检索文档进行过滤, 随后返回文档摘要, 此种架构十分适合知识类问答. 但对有逻辑类问题处理不佳.</p><h2 id=结合公司使用场景分析>结合公司使用场景分析
<a class=anchor href=#%e7%bb%93%e5%90%88%e5%85%ac%e5%8f%b8%e4%bd%bf%e7%94%a8%e5%9c%ba%e6%99%af%e5%88%86%e6%9e%90>#</a></h2><p>再了解到以上知识之后, 可以得出以下几点：</p><ol><li>数据处理需要处理哪些如何处理还不太了解</li><li>通过数据做报告是ChatGPT的强项, 不过对于输出格式每次输出都可能采取不同的格式, 每次关注的重点和排版都可能会有所变化. 如果使用的话还是需要多次的Prompt调试, 寻找到输出结果稳定, 符合我们预期的Prompt. 　同时模型可能会对数据有所遗漏, 这个不太可控.
－　对于代码安全优化的使用, 一是模型确实可以检测代码中的漏洞, 也可以对代码进行优化.
－　二是模型可能会输出有漏洞的代码, 而我们再次反问它生成的代码是否有漏洞时, 它也会坦然承认</li><li>敏感数据识别、威胁语义检测、威胁定性分析、引导式运营分析、报告生成、以及处置建议都属于大模型适合处理的内容.</li><li>在攻击端的使用可能会有大的阻力, 因为生成钓鱼邮件和攻击内容, 是被模型训练方不断弱化的内容. 成熟的大模型都不会给出攻击建议.</li><li>知识提供也是适合模型去完成的内容.</li></ol><p>综上, 在知识类问答中可以参考<code>privateGPT</code>的实现方式, 提供稳定的知识问答. 而在数据分析上则不可采用<code>privateGPT</code>的方法. 应该寻找效果好的模型进行<code>Prompt tuning</code>或者<code>LoRA</code>训练. 无论通过什么方法实现, 我们也应该同时注重<code>模型基建</code>. 这类<code>基建</code>任务, 无论是从头开始训练大模型还是在<code>Prompt</code>应用, 以及<code>LoRA</code>训练上都占有不下于百分之七十的工作量.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#先介绍几个名词>先介绍几个名词</a><ul><li><a href=#gpt-和-chatgpt>GPT 和 ChatGPT</a></li><li><a href=#llms>LLMs</a></li><li><a href=#lora和prompt>LoRA和Prompt</a></li></ul></li><li><a href=#llms如何产生思维>LLMs如何产生“思维”</a><ul><li><a href=#我们如何使用>我们如何使用</a></li><li><a href=#目前一些有有影响力的开源模型>目前一些有有影响力的开源模型</a></li></ul></li><li><a href=#privategpt-和-localgpt>privateGPT 和 localGPT</a></li><li><a href=#结合公司使用场景分析>结合公司使用场景分析</a></li></ul></nav></div></aside></main></body></html>